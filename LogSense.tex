\documentclass{report}
\usepackage{titlesec}

\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries}{}{0pt}{\Huge}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{todonotes}
\usepackage{babel}
\usepackage[numbers,comma,square]{natbib}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{grffile}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\bibliographystyle{plainnat}
\setdescription{leftmargin=0pt}


\pagestyle{fancy}
\fancyhf{}
\rhead{LogSense}
\lhead{HTL Perg}
\renewcommand{\headrulewidth}{0pt}



\fancypagestyle{plain}{
  \fancyhf{}
  \rhead{LogSense}
  \lhead{HTL Perg}
  \fancyfoot[R]{\thepage}
  \fancyfoot[L]{Borbely, Ettlinger, Jilek, Stadlbauer}
  \renewcommand{\headrulewidth}{0pt}
}


\fancyfoot{}
\fancyfoot[R]{\thepage} % Page number format on the right side
\fancyfoot[L]{Borbely, Ettlinger, Jilek, Stadlbauer} % Names on the left side
\begin{document}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{LatexStyleFiles/htl_perg.png}
    \caption{Höhere Lehranstalt für Informatik}
\end{figure}

\renewcommand{\arraystretch}{1.5}
\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \textbf{Thema:} & LogSense - Infrastruktur-Monitoring-Appplikation \\
        \textbf{Eingereicht von:} & Philipp Borbely \textless philipp.borbely@gmail.com\textgreater \\
                                   & Thomas Jilek \textless thomas.jilek@gmail.com\textgreater \\
                                   & Sarah Ettlinger \textless sarah.ettlinger@gmail.com\textgreater \\
                                   & Emily Stadlbauer \textless emily.stadlbauer@gmail.com\textgreater \\
        \textbf{Eingereicht am:} & 05. April 2024 \\
        \textbf{Betreuer:} & Prof. Maria Inreiter, MSc \\
        \textbf{In Zusammenarbeit mit:} & Dynatrace
    \end{tabular}
\end{table}

\tableofcontents

\chapter*{Eidesstattliche Erklärung}
This chapter introduces the background and motivation behind the software engineering project.

\chapter*{Gender-Erklärung}
Define the scope of the project, including any limitations and constraints.

\chapter*{Danksagung}
Summarize relevant literature and related work in the field of software engineering.

\chapter*{Impressum}
Clearly state the problem the project aims to address.

\chapter*{Kurzfassung}
Die Grundidee von LogSense ist es die Bildschirmzeitverwaltung von Smartphones und Teile der Funktionalität des Task Managers vom Betriebssystem Windows zu fusionieren. Als erweiterte Funktionalität sollen Probleme wie ein hoher Ressourcenverbrauch durch einen bestimmten Prozess erkannt und dem Benutzer mitgeteilt werden. Ein wesentlicher Punkt ist auch die langfristige Speicherung der Daten, um sie rückblickend betrachten und analysieren zu können.\\\\

\noindent Um dieses Ziel zu erreichen, werden 4 Hauptkomponenten benötigt:\\\\
Der Agent läuft als Hintergrundprozess auf dem Client Rechner, erfasst Daten über die Hardware-Komponenten des Rechners und sendet sie zur weiteren Analyse an den Server. Bei der ersten Datenerfassung werden zusätzlich allgemeine Daten über den Rechner erfasst.\\\\
In der Datenbank werden die erfassten Informationen für die spätere Auswertung persistiert. Da es sich dabei um eine große Menge an Zeitreihen-Daten handelt, muss eine leistungsstarke Datenbank eingesetzt werden. Dafür eignet sich am besten die Datenbank TimescaleDB, da sie genau für diesen Anwendungszweck entwickelt worden ist.\\\\
Die gemessenen Daten werden serverseitig durch Machine-Learning Algorithmen und statistische Verfahren analysiert. Dazu zählt die Identifizierung von Anomalien, Ereignisse im Trendverlauf und die Vorhersage von freiem Speicherplatz.\\\\
Die Benutzeroberfläche zeigt die erfassten Daten und die Ergebnisse der Analyse in Form von Diagrammen und Statistiken übersichtlich an. Zusätzlich kann der Benutzer über die Benutzeroberfläche mit dem System interagieren und zum Beispiel eigens definierte Warnungen für ein Gerät erstellen.\\\\

\noindent Das Ergebnis ist ein System, bestehend aus den oben angeführten Komponenten, mit dem der Ressourcenverbrauch von Rechnern, sowie die Laufzeit und der Ressourcenverbrauch von einzelnen Prozessen überwacht werden kann. Die gesammelten Daten werden analysiert und ausgewertet, um in Form von Statistiken, Events und Anomalien dargestellt zu werden.


\chapter*{Abstract}
The basic idea of LogSense is to merge the screen time management of smartphones and some functionalities of the task manager from the Windows operating system. As an additional feature, unexpected behaviours such as high resource consumption of particular processes can be detected and reported to the user. Another key feature is the long-term storage of data so that it can be viewed and analyzed retrospectively.\\\\

\noindent To achieve this goal, 4 main components are required:\\\\
The agent runs as a background process on the client computer, collects data about the hardware components of the PC and sends it to the server for further analysis. During the initial data collection, general information about the computer is gathered.\\\\
The recorded information is then persisted in the database for further analysis. As this involves large amounts of time series data, a powerful database is required. The database most suitable for this is TimescaleDB, as it was developed precisely for this purpose.\\\\
The measured data is analyzed on the server using machine learning algorithms and statistical methods. This includes the identification of anomalies, trending events and the prediction of free storage space.\\\\
The graphical user interface displays the recorded data and the analysis results in form of diagrams and statistics. In addition, the user can interact with the system via the user interface and for example, define custom warnings for a device.\\\\

\noindent The result is a system consisting of the components listed above, which can be used to monitor the resource usage of computers as well as the runtime and resource consumption of individual processes. The collected data is analyzed and evaluated in order to be displayed in the form of statistics, events and anomalies.


\chapter*{Inhaltsverzeichnis}

\chapter{Einleitung}
\section{Problemstellung}
Beim Arbeiten mit Rechner können vielerlei Probleme in Bezug auf Applikationen und Hardware auftreten. Allerdings können Probleme nur erkannt werden, wenn Benutzer direkt mit dem betroffenen Rechner interagieren, daher können Systemadministratoren nicht an einer zentralen Stelle Probleme alleine identifizieren. Außerdem können mithilfe von bereitgestellten Tools wie dem Windows-Taskmanager keine rückblickenden Daten über den Hardware-Verbrauch oder die Verwendungsdauer von Applikationen eingesehen werden.
\section{Zielsetzung}
Das Ziel unserer Diplomarbeit ist es, den Benutzer eines Rechners und in größeren Rechnernetzen auch dessen Administrator, eine Übersicht über den Ressourcenverbauch eines PCs und dessen Applikationen zur Verfügung zu stellen. Die Applikation soll ermöglichen, vergangene Hardware-Metriken über Applikationen einzusehen um potentielle Trends oder ungewöhnliches Verhalten zu erkennen. Auch sollten Analysemöglichkeiten zum Identfizieren von Anomalien und Ereignissen im Trendverlauf angeboten werden. Im Allgemeinen sollte dadurch das Auftreten von Problemen frühzeitig erkannt werden und dessen Behandlung durch das Bereitstellen von relevanten Informationen vereinfacht werden.
\section{Projektinhalt}
Eine übersichtliche Projektstruktur ist eine Notwendigkeit, um die Projektziele zu erreichen. Die Diplomarbeit besteht aus den folgenden 5 Hauptkomponenten:
\subsection{Agent}
Der Agent erfasst alle 10 Sekunden Daten über die Hardware-Komponenten eines Rechners, auf dem das Betriebssystem Windows läuft. Nach der Datenerfassung fasst der Agent die Daten der letzten 60 Sekunden zusammen und bereitet sie auf, bevor er sie zur weiteren Analyse an den Server sendet. Bei der ersten Datenerfassung werden zusätzlich allgemeine Daten über den Rechner erfasst. Die erste Nachricht signalisiert den Start einer Session.
\subsection{Analyse} 
Die gemessenen Daten werden serverseitig durch Machine-Learning Algorithmen und statistische Verfahren analysiert. Dazu zählt die Identifizierung von Anomalien, also von Datenpunkte, die enorm von den restlichen Daten abweichen. Außerdem werden sogennante Change-Points erkannt, also Datenpunkte, welche Ereignisse im Trendverlauf darstellen. Zusätzlich gibt es Möglichkeiten, zukünftigen freien Speicherplatz vorherzusagen.
\subsection{Schnittstellen}
Eine RESTful API wird für die Kommunikation zwischen den einzelnen Komponenten verwendet. Die Schnittstelle ist für die Speicherung der vom Agent gesendeten Daten verantwortlich. Außerdem nimmt die Schnittstelle Anfragen der Benutzeroberfläche entgegen und fordert bei Bedarf die analysierten Daten an. Danach werden die ausgewerteten Daten an die Webanwendung weitergeleitet.  
\subsection{Datenspeicherung} 
In der Datenbank werden die erfassten Informationen für die spätere Auswertung persistiert. Da es sich dabei um eine große Menge an Zeitreihen-Daten handelt, muss eine leistungsstarke Datenbank eingesetzt werden. Dafür eignet sich am besten die Datenbank TimescaleDB, da sie genau für diesen Anwendungszweck entwickelt worden ist.
\subsection{User Interface}
Die Benutzeroberfläche zeigt die analysierten Daten in Form von Diagrammen und Statistiken übersichtlich an. Beim erstmaligen Aufrufen der Benutzeroberfläche kann ein Gerät ausgewählt werden, dessen Daten dargestellt werden. Weiters können die Benutzer eigens definierte Warnungen für ein Gerät erstellen, indem sie die Art der Diagnose und den jeweiligen Grenzwert angeben.
\section{Projektumfeld}
An dieser Diplomarbeit sind mehrere Personen beteiligt und werden in dem nachfolgenden Abschnitt angeführt.
\subsection{Projektteam}
Das Projektteam besteht aus 4 Schüler:innen der HTL Perg und die Verantwortlichkeiten haben sich dabei wie folgt aufgeteilt:
\begin{itemize}
    \item Philipp Borbely - Datenanalyse und -auswertung
    \item Sarah Ettlinger - Datenerfassung
    \item Thomas Jilek - Datenspeicherung und Systemintegration
    \item Emily Stadlbauer - Visualisierung und Anzeige
\end{itemize}
\subsection{Auftraggeber}

\chapter{Theoretische Grundlagen}
\section{Visualisierung}
 In den nachfolgenden Abschnitten werden die im Projekt verwendeten Konzepte und Technologien  für die Visualisierung beschrieben.
\subsection{Angular}
Angular ist ein Framework zur Entwicklung von Single Page Applications. Es verwendet eine komponentenbasierte Architektur. Diese Komponenten sind eigenständige und wiederverwendbare Bestandteile  der Applikation, die Struktur in die Anwendung bringen. Eine Komponente besteht aus einer HTML-, einer CSS- und einer TypeScript-Datei. Mithilfe von Dependency Injection können Dienste Daten oder Funktionen  zwischen den einzelnen Komponenten geteilt werden. Weiters wird ein Angular Projekt mit Modulen organisiert. Ein Modul besteht aus beliebig vielen Komponenten und Diensten. Angular bietet zudem ein leistungsfähiges Routing-Modul für die Navigation zwischen verschiedenen Ansichten. Um Daten aus einer REST API zu laden, ist in Angular ein HTTP-Client enthalten, der es ermöglicht Daten auszutauschen. Um das Verhalten der einzelnen DOM-Elemente zu steuern werden Direktiven verwendet.
Einweg- und Zweirichtungsdatenbindung ermöglicht es dynamische Daten im HTML zu verwenden.
\\In dieser Diplomarbeit wird Angular verwendet um die gesammelten und analysierten Daten in einer Webapplikation zu visualisieren.

\subsection{BootStrap}
Bootstrap ist ein CSS Framework um schnell und einfach responsive Webseiten zu erstellen. Es bietet unterschiedliche, vordefinierte Komponenten, die angepasst und eingebunden werden können. Ein responsives Rastersystem, das in Bootstrap inkludiert ist, hilft flexible Layouts zu erstellen. 

Mithilfe von Sass können Komponenten importiert werden und globale Variablen, wie Schatten und Farben, definiert werden. Weiteres bietet SASS Funktionen zur Durchführung von Berechnungen und Mixins zur Wiederverwendung von CSS-Regeln. Vorhandene Komponenten können auf eigen Bedürfnisse angepasst werden.

In dieser Diplomarbeit wird Bootstrap 5 verwendet um die Webapplikation responsiv zu gestalten und mithilfe von SASS-Variablen die CSS-Klassen übersichtlicher zu strukturieren.

\subsection{Charst JS}
Chart.js ist eine JavaScript Bibliothek mit deren Hilfe unterschiedliche Diagrammtypen in Webanwendungen dargestellt werden können. 
Unteranderem sind folgende Diagrammarten definiert: \newpage

\begin{table}[htbp]
  \centering
  \begin{tabular}{cc}
    \subcaptionbox{Lininendiagramm}{\includegraphics[width=0.4\textwidth]{ChartJS/line.png}} &
    \subcaptionbox{Balkendiagramm}{\includegraphics[width=0.4\textwidth]{ChartJS/bar.png}} \\
    \subcaptionbox{Bereichsdiagramm7}{\includegraphics[width=0.4\textwidth]{ChartJS/radar.png}} &
    \subcaptionbox{Streudiagramm}{\includegraphics[width=0.4\textwidth]{ChartJS/scatter.png}} \\
  \end{tabular}
  \caption{ChartJS Beispiele}
  \label{tab:bilder}
\end{table}


Die Bibliothek bietet unterschiedliche, interaktive Funktionen wie Tooltipps und Animationen. 
Darüber hinaus gibt es viele Möglichkeiten mit verschiedenen Optionen und Konfigurationen das Diagramm nach eigenen Wünschen  anzupassen. 
Chart.js unterstützt responsives Design und kann sich an unterschiedliche Bildschirmgrößen anpassen.\\In dieser Diplomarbeit ist Chart.js im Angular Projekt eingebunden um unterschiedliche Messwerte grafisch darzustellen. 

\subsection{Angular Material}
Angular Material ist eine Bibliothek mit UI-Komponenten für Angular Projekte. Sie bietet verschiedene vorgefertigte UI-Komponenten die innerhalb der Design-Spezifikationen angepasst werden können. Einige dieser Komponenten sind:
\begin{itemize}
    \item Icons
    \item Inputs
    \item Radio Buttons
    \item Slider
\end{itemize}
Die UI-Komponenten können sich unterschiedlichen Bildschirmgrößen anpassen. Angular Material ist für Angluar-Projekte ausgelegt und verwendet Angular-spezifische Funktionen und lässt sich einfach in Angular-Projekte integrieren.
In dieser Diplomarbeit werden Angular Material Komponenten verwendet um die Benutzeroberfläche ansprechender zu gestalten.

\section{Datenverarbeitung}
\subsection{Python}
\subsection{Pandas}
Pandas ist eine Python-Bibliothek zur Datenverarbeitung und -analyse. Sie stellt verschiedene Datenstrukturen und Funktionen für das Manipulieren von tabellarischen und strukturierten Daten zur Verfügung. Zu den wichtigsten Strukturen gehören DataFrames und Series.
\begin{itemize}
    \item \textbf{Series} \\
    \begin{minipage}[t]{\linewidth}
        Series sind eindimensionale, array-ähnliche Objekte, die wie eine einzelne Spalte in einer Tabelle angeordnet sind.
    \end{minipage}

    \item \textbf{DataFrames} \\
    \begin{minipage}[t]{\linewidth}
        Bei DataFrames handelt es sich um zweidimensionale Tabellen mit Zeilen und Spalten. Einzelne Spalten von einem DataFrame können dabei verschiedene Datentypen aufweisen. Zu den wichtigsten Funktionen gehören Aggregationen, Gruppierungen und Statistische Operationen. 
    \end{minipage}
\end{itemize}
  In der vorliegenden Diplomarbeit wird Pandas bei der Datenverarbeitung eingesetzt, insbesondere in Kombination mit Scikit-Learn, Numpy und Ruptures.
  Features bei denen Pandas eingesetzt wird:
  \begin{itemize}
      \item Anomalien
      \item Change-Points
      \item Bereinigung von Datensätzen
      \item Aggregation von Datensätzen
  \end{itemize}

\subsection{Numpy}
\subsection{Scikit-Learn}
\subsection{Ruptures}
\subsection{Algorithmen}

\section{Datenerfassung}
In den nachfolgenden Abschnitten werden die im Projekt verwendeten Konzepte und Technologien für die Datenerfassung beschrieben.

\subsection{Agent}
Für die Erfassung der Ressourcendaten wird ein sogennanter Agent verwendet. Dieser kann selbstständig, das heißt ohne das Zutun eines Benutzers oder eines anderen Programmes, Tätigkeiten und Prozesse ausführen. Das heißt, der Agent wartet so lange bis eine gewisse Zeit abgelaufen, ein Programm gestartet worden oder ein Ereignis jeglicher Art festgestellt worden ist und erledigt darauf bestimmte Aufgaben.\\
Dabei gibt es im wesentlichen 3 Arten von Agenten:
\begin{itemize}
    \item \textbf{Reaktive Agenten}
        \begin{description}
            \item \noindent Reaktive Agenten beobachten die Umgebung, auf der sie laufen und treffen Entscheidungen basierend auf den erfassten Daten.
        \end{description}
    \item \textbf{Adaptive Agenten}
        \begin{description}
            \item \noindent Diese Art von Agenten handelt, im Vergleich zu reaktiven Agenten, zusätzlich basierend auf bereits zuvor erfassten Daten und erkannten Zusammenhängen zwischen den Informationspunkten.
        \end{description}
    \item \textbf{Kognitive Agenten}
        \begin{description}
            \item \noindent Kognitive Agenten können weiters aus den erfassten Daten Muster lernen und selbstständig abwägen, welche Aufgaben wann und wie durchgeführt werden sollen, um bestmöglich auf die aktualle Situation der Umgebung reagieren zu können.
        \end{description}
\end{itemize}
Für diesen Anwendungsfall wird ein reaktiver Agent verwendet. Nach Ablauf von 60 Sekunden werden die Daten gemessen, je nach Art der Daten zusammengefasst und anschließend in einem geeigneten Format zur Auswertung an den Server gesendet. Es wird also überprüft, um welche Daten es sich handelt und daraufhin entschieden wie die Daten für das Senden vorbereitet werden.

\subsection{Java}
Die Programmiersprache Java ist objektorientiert, besitzt eine einfache Syntax mit überschaubarem Sprachumfang und ist durch kontrollierte Speicherzugriffe sicher. Wenn es eine passende JVM (= Java Virtual Machine) für das Betriebssystem gibt, kann das Programm auf diesem Betriebssystem laufen. Diese Plattformunabhängigkeit wird dadurch ermöglicht, dass der Java-Quellcode in Bytecode übersetzt wird, der dann von der JVM interpretiert werden kann.\\
Der Agent, der lokal auf den Windows Rechnern läuft und die Daten über den Ressourcenverbrauch des Rechners erfasst, ist in der Programmiersprache Java entwickelt. Der Grund dafür ist, dass es für Java viele verschiedene Bibliotheken gibt, die auf die Hardwaretreiber zugreifen, um die benötigten Daten über den Rechner, die darauf laufenden Programme und deren Ressourcenverbrauch auszulesen.

\subsection{Oshi}
Oshi\footnote{\url{https://www.oshi.ooo/}} (= Operating System and Hardware Information) ist eine für die Programmiersprache Java entwickelte Bibliothek, die Daten über die Hardware und das Betriebssystem eines Rechners erfassen und bereitstellen kann. Dafür wird im Hintergrund die JNA-Bibliothek (= Java Native Access) verwendet. Diese dient als eine plattformunabhängige Schicht zwischen dem Programmcode der Oshi-Bibliothek und dem nativen Code, der abhängig vom Betriebsystem die Systeminformationen ausliest. Aus diesem Grund kann Oshi Daten über unterschiedliche Betriebssysteme, unter anderem Windows, Linux, macOs und verschiedene Unix Distributionen, erfassen.

\section{Datenhaltung}
\subsection{Postgres}
\subsection{Timescale DB}

\section{Schnittstellen}
\subsection{Fast API}
\subsection{REST}

\section{Server}
\subsection{Ubuntu}
\subsection{nginx}

\section{Entwicklungssysteme}
\subsection{PyCharm}
\subsection{IntelliJ IDEA}
IntelliJ IDEA\footnote{\url{https://www.jetbrains.com/idea/}} ist eine vom Unternehmen JetBrains entwickelte IDE (= Integrated Development Environment). Diese funktioniert als Code-Editor vor allem für jene Programmiersprachen, die auf der JVM (= Java Virtual Machine) basieren, wie Java, Kotlin, Scala und Groovy. Wobei auch andere Programmiersprachen, Frameworks und Dateitypen unterstützt werden. Zum Beispiel Python, SQL, HTML, Angular, XML und Markdown. Zusätzlich bietet IntelliJ diverse Funktionen, die den Entwicklungsprozess von Anwendungen wesentlich vereinfachen. Dazu zählen Versionsverwaltung, Code-Vervollständigung, Refactoring und die Unterstützung von verschiedensten Build-Tools.\\\\
Für die Implementierung des Agents in der Programmiersprache Java wurde IntelliJ IDEA Ultimate in der Version 2023.2 verwendet.

\subsection{Webstorm}
WebStorm ist eine Entwicklungsumgebung speziell für Webentwicklung von JetBrains. Die IDE bietet Unterstützung für unterschiedliche Webtechnologien wie HTML, CSS, JavaScript, TypeScript und Node.js. Sie enthält Funktionen wie Code-Verollständigung, Refakotirierung undn Syntax-Hervorhebung ebenso wie ein integriertes Versionskontrollsystem. Dieses ermöglicht es Entwicklern direkt in der IDE Änderungen zu verfolgen und Commits durchzuführen. Durch eine Live-Edit-Funktion werden Änderungen die am Code vorgenommen werden direkt im Browser angezeigt. Weiters sind in WebStorm Code-Standards enthalten. Mit integrierten Tools wird der Code automatisch auf seine Qualität geprüft.
In dieser Diplomarbeit wird das Angular-Projekt in WebStorm entwickelt.

\subsection{Datagrip}
\section{Sonstige Software}
\subsection{GitLab}
GitLab ist eine Open-Source-Software für das Managen von Versionskontrollsystemen und bietet Funktionen für die Zusammenarbeit in Softwareprojekte. Git ermöglicht es Änderungen im Code zu verfolgen und nachzuvollziehen. In Repositories wird der Code hochgeladen und geteilt, damit mehrere Personen an einem Projekt arbeiten können. Weiters unterstützt GitLab Issue-Tracking, Continuous Integration und Delivery sowie DevOps-Prozesse.
\\
In dieser Diplomarbeit wird das, von der HTL Perg gehostete, GitLab verwendet, auf dem der Code von LogSense liegt, um die Zusammenarbeit beim Entwickeln des Projektes zu erleichtern.
\subsection{Adobe XD}
Adobe XD ist ein Design- und Prototyping-Tool, das von Adobe Inc. entwickelt wurde. Es ermöglicht das einfache Erstellen von Benutzeroberflächen und Benutzererfahrungen für Websites und mobile Apps. Von der Prototypenerstellung bis zur Simulation von Interaktionen bietet Adobe XD alles um eine Webapplikation oder eine mobile App zu entwerfen. 
\\Während der Planung dieser Diplomarbeit haben wir Adobe XD genutzt um die Benutzeroberfläche unserer Webapplikation zu entwerfen.
\subsection{LucidChart}
\subsection{StarUML}
StarUML ist eine Software, die zur Modellierung von Software und Systemen verwendet wird. Es bietet eine Vielzahl an Funktionen zur Erstellung von UML-Diagrammen. Es unterstützt Entwickler bei der Darstellung von Struktur und Verhalten von Software. Fast alle UML-Diagrammarten werden von StarUML unterstützt, unteranderem UseCase-Diagramme, Sequenz-Diagramme und Klassendiagramme.
\\
Während der Planung dieser Diplomarbeit haben wir StarUML verwendet um UseCase-Diagramme zu erstellen.

\chapter{Planung und Realisierung}
\section{Funktionalität}
LogSense kann in drei Kern-Funktionalitäten aufgeteilt werden welche in den drei Use-Case-Diagrammen PC-Auswahl, Datenabruf und Custom-Alert dargestellt werden.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{UC/Datenabruf.png}
    \caption{Datenabruf Use-Case-Diagramm}
\end{figure}
In der obigen Grafik wird der Use-Case "Datenabruf" abgebildet. Der Benutzer kann Auslastungsdiagramme von RAM, CPU oder freien Speicher aufrufen. Danach gibt es die Möglichkeit, einen Zeitraum für das Laden der Daten auszuwählen sowie die Möglichkeit, ein Zeitintervall anzugeben, in dem die Daten gruppiert werden sollen. Zusätzlich kann der Benutzer alle aufgetretenen Anomalien und Events im Graphen einblenden. Wenn der Benutzer seinen Zeiger über eine Anomalie oder Event bewegt, werden Statistiken zu diesem Event beziehungsweise der Anomalie angezeigt, unter anderem die Applikationen die zu diesem Zeitpunkt die größte Auslastung hatten. Der Agent ließt den Hardware-Verbrauch vom Client-Rechner ein und sendet die gemessenen Daten zur Rest-API welche die Daten auf der Datenbank persistiert. Der Benutzer kann über die Benutzeroberfläche diese Daten über die Rest-API anfordern. Die Rest-Schnittstelle widerum kommuniziert mit der Datenbank um die benötigten Daten zu laden und analysiert diese darauffolgend mit den implementierten Algorithmen im Backend. 
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{UC/PC_Selection_UseCase.png}
    \caption{PC-Auswahl Use-Case-Diagramm}
\end{figure}
Dieser Use-Case stellt den Prozess der PC-Auswahl dar. Zuerst wird dem Benutzer eine Liste von PCs angezeigt. Von dieser Liste hat dieser dann die Möglchkeit, einen Rechner auszuwählen. Es gibt die Möglichkeit, einen neuen PC hinzuzufügen. Die Liste der PCs wird über ein API-Anfrage angefordert wobei die Rest-API die benötigten Daten aus der Datenbank lädt. Beim Hinzufügen eines neuen Rechners werden die Rechner-Informationen per API-Anfrage übermittelt und auf der Datenbank gespeichert.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{UC/CustomAlert_UseCase.png}
    \caption{Custom Alert Use-Case-Diagramm}
\end{figure}
Nach dem Aufrufen des Custom-Alert-Screens wird dem Benutzer eine Liste von bestehenden Custom-Alerts angezeigt. Bei der Auswahl eines Alerts werden die Details von diesem angezeigt. Danach kann dieser auch bearbeitet oder gelöscht werden. Der Benutzer kann auch einen neuen Custom-Alert anlegen. Bei Anzeige der Custom-Alerts wird eine API-Anfrage per Rest gesendet und die angeforderten Daten werden aus der Datenbank geladen. Beim Anlegen oder Bearbeiten eines Alerts werden die Informationen über die Rest-API verschickt und auf der Datenbank gespeichert. 


\section{Entwurf}
In den nachfolgenden Abschnitten wird der Prototyp der Benutzeroberfläche mit seinen Funktionalitäten dargestellt.
\subsection{Startseite}
Die finale Version des Entwurfs unterscheidet sich deutlich von der ersten Idee. Vor allem kann man erkennen, dass alle relevanten Details in der finalen Version bereits auf der Startseite angezeigt werden. Beim Hovern über das Info-Icon werden zum Beispiel sämtliche Informationen über den Rechner angeführt. Außerdem werden die aktuellen Statistiken zum Ressourcenverbrauch angezeigt.
Die Gesamtdauer der Prozesse, die in einem auszuwählbaren Zeitbereich gestartet worden sind, werden nur auf der Startseite in einem Balkendiagramm dargestellt.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_startseite.png}
    \caption{finaler Entwurf der Startseite}
\end{figure}
\noindent Die 1. Version des Entwurfs ist überarbeitet worden, weil zu viele Informationen auf zu kleinem Raum zusammen gequetscht sind. So konnte verhindert werden, dass wichtige Informationen zu wenig Platz bekommen.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_startseite_alt.png}
    \caption{1. Version des Entwurfs der Startseite}
\end{figure}
\subsection{Ressourcenverbrauch}
Für jede einzelne Ressource gibt ein eigene Ansicht in der Webanwendung.
Die CPU-Auslastung in einem auszuwählenden Zeitraum wird in einem Liniendiagramm dargestellt. Dabei gibt es die Möglichkeit den Zeitabstand zwischen den Datenpunkten festzulegen. Außerdem kann der Benutzer auswählen, ob im Diagramm auch Events und Anomalien markiert werden sollen.
Weiters werden Statistiken der analysierten Daten wie die Stabilität und der Durchschnittsverbrauch angezeigt.
CPU-spezifische Alerts und vom Benutzer angelegte Alerts, die sich auch die CPU beziehen, werden ebenfalls in dieser Ansicht aufgelistet.
Auch die generellen Informationen über die CPU sowie die CPU-Auslastung der einzelnen Prozesse werden hier angezeigt.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_cpu.png}
    \caption{finaler Entwurf der CPU-Ansicht}
\end{figure}
Fast wie bei der CPU-Ansicht werden hier der RAM-Verbrauch, die Statistiken der analysierten Daten, Alerts und die RAM-Auslastung der einzelnen Prozesse abgebildet. Statt den generellen Informationen werden hier die Hauptspeichergröße, der freie Speicherplatz und die Größe einer Speicherseite angegeben.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_ram.png}
    \caption{finaler Entwurf der RAM-Ansicht}
\end{figure}
Auf der Festplatten-Ansicht sieht man den Verlauf des freien Speichers in einem auszuwählenden Zeitraum in einem Liniendiagramm. Wie auch bei den oben angeführten Ressourcen kann man den Zeitabstand zwischen den Datenpunkten festlegen. Zusätzlich kann im Liniendiagramm eine Vorhersage des freien Speichers in den nächsten Tagen eingeblendet werden.
Auch in dieser Ansicht werden die Festplatten-spezifischen Alerts aufgelistet. Generelle Informationen zu allen erkannten Speichermedien wie zum Beispiel das Modell und der Gesamtspeicherplatz können hier eingesehen werden. Zu jedem dieser Speichermedien können weitere Informationen über deren Partitionen eingeholt werden.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_disk.png}
    \caption{finaler Entwurf der Festplatten-Ansicht}
\end{figure}
Um dem Liniendiagramm mehr Raum zu geben wurde der erste Designentwurf nochmals überarbeitet. Dadurch können die Informationen in den Graphen besser entnommen werden. Im ersten Entwurf wurde auch die Auswahl der Zeit zwischen den Datenpunkten noch nicht berücksichtigt.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_cpu_erste_version.png}
    \caption{1. Version des Entwurfs der Ressourcen-Ansicht}
\end{figure}

\subsection{Prozesse}
Alle Prozesse, die im ausgewählten Zeitraum erfasst worden sind, werden in dieser Ansicht aufgelistet. In der Detail-Ansicht eines konkreten Prozesses werdne die CPU- und RAM-Auslastungen in dem ausgewählten Zeitraum in Liniendiagrammen dargestellt. Dabei gibt es die Möglichkeit den Zeitabstand zwischen den Datenpunkten festzulegen. Außerdem besteht die Möglichkeit im Diagramm erkannte Events und Anomalien zu markieren.
Sämtliche Alerts, die den ausgwählten Prozess betreffen werden hier aufgelistet.
Zusätzlich werden generelle Informationen, zum Beispiel der Status und die Anzahl der Threads, und Statistiken der analysierten Daten angezeigt.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_processes.png}
    \caption{finaler Entwurf der Prozess-Ansicht}
\end{figure}
In der 1. Version des Entwurfs sind die beiden Liniendiagramme nebeneinander positioniert, somit haben sie weniger Platz zur Verfügung und die Inforationen können schlechter entnommen werden. Außerdem sind bei den Statistiken die Werte der Stabilität und des Durchschnittsverbrauch entfernt worden, weil die meisten Prozesse so kurz laufen, dass diese Statistiken keine große Aussagekraft haben.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_prozesse_erste_version.png}
    \caption{1. Version des Entwurfs der Prozess-Ansicht}
\end{figure}

\subsection{Netzwerkschnittstellen und Verbindungen}
Auf dieser Ansicht werden alle virtuellen und physischen Netzwerkschnittstellen, die ein Rechner besitzt, angezeigt. Alle aktuellen Netzwerk-Verbindungen zu dem ausgewählten Rechner werden ebenfalls detailiert aufgelistet.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_network.png}
    \caption{finaler Entwurf der Netzwerk-Ansicht}
\end{figure}

\subsection{Benutzerdefinierte Alerts}
In dieser Ansicht gibt es die Möglichkeit benutzerdefinierte Alerts anzulegen und zu bearbeiten. Dafür können folgende Eigenschaften definiert werden:
\begin{itemize}
    \item \textbf{Titel}
    \item \textbf{Nachricht}
        \begin{description}
            \item \noindent Die Nachricht wird beim Auftreten des Alerts in der jeweiligen Ansicht der Webanwendung angezeigt.
        \end{description}
    \item \textbf{Wichtigkeitsgrad}
    \item \textbf{Ressource}
        \begin{description}
            \item \noindent Es kann ausgwählt werden welche Ressource der Alert überwacht. Zur Auswahl stehen CPU, RAM und verschiedene Eigenschaften der Festplatte.
        \end{description}
    \item \textbf{Anwendung}
        \begin{description}
            \item \noindent Hier kann definiert werden welche Anwendung der Alert überwacht. Der gesamte Rechner wird überwacht, wenn keine Anwendung ausgewählt wird.
        \end{description}
    \item \textbf{Grenzwert}
        \begin{description}
            \item \noindent Der Grenzwert kann als prozentueller und auch als absoluter Wert angegeben werden. Wenn beide Werte definiert werden, greift der Grenzwert, der als erster überschritten wird.
        \end{description}
    \item \textbf{Vergleichsoperator}
        \begin{description}
            \item \noindent Durch den Vergleichsoperator kann festgelegt werden, ob der Alert auftritt, wenn der gemessene Wert unter, gleich oder über dem Grenzwert liegt.
        \end{description}
    \item \textbf{Erkennungsmethode}
        \begin{description}
            \item \noindent Der Benutzer kann zwischen "Moving Averages" und ... auswählen, nach welcher Methode der Alert erkannt wird.
        \end{description}
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_custom_alerts.png}
    \caption{finaler Entwurf der benutzerdefinierten Alerts}
\end{figure}

\subsection{Rechner-Auswahl}
Bevor die Daten über den Rechner angezeigt werden können, muss zuerst ein Rechner ausgewählt werden. Dafür wird der Name des Rechners und die Hardware UUID einmalig hinzugefügt und kann in weiterer Folge immer wieder selektiert werden. Nach dem Festlegen eines Rechners stehen auf den zuvor angeführten Ansichten die zugehörigen Daten zur Verfügung.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_pc_selection.png}
    \caption{finaler Entwurf der Rechner-Auswahl}
\end{figure}
\noindent Die 1. Version wurde überarbeitet, weil keine Detail-Ansicht zu einem Rechner gebraucht wird und es dennoch eine Möglichkeit geben soll, Rechner auszuwählen und zu entfernen.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_pc_selection_alt.png}
    \caption{1. Version des Entwurfs der Rechner-Auswahl}
\end{figure}

\subsection{Gruppenanzeige}
Anfänglich ist die Überlegung im Raum gestanden, dass Admins mehrere Rechner in Gruppen einteilen können, um eine große Anzahl an Rechnern effizient zu verwalten. Alle Ansicht würden bei dieser Version minimal angepasst werden. Als Beispiel werden hier 3 Ansichten für Gruppen angeführt.
In jeder Ansicht befindet sich auf der linken Seite eine Auswahlmöglichkeit für Gruppen beziehungsweise deren zugeordneten Rechnern. In der Detail-Ansicht einer Gruppe werden Informationen zu den Rechnern und deren Status angeführt. Zusätzlich werden Warnungen von allen Rechnern der Gruppe hier aufgelistet.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_startseite_gruppen.png}
    \caption{Entwurf der Detail-Ansicht einer Gruppe}
\end{figure}
\noindent In der Ressourcen-Ansicht gibt es, wie zuvor beschrieben, links die Auswahlmöglichkeit für Gruppen, Rechners und einzelnen Ressourcen wie RAM, CPU, Festplatte und Netzwerk. Hier wird ein konkretes Beispiel für die Informationen der CPU des ausgwählten Rechners angeführt.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_ressourcen_gruppen.png}
    \caption{Entwurf der Anzeige des Ressourcenverbrauchs für die Gruppenanzeige}
\end{figure}
\noindent Natürlich besteht auch die Möglichkeit Informationen über die laufenden Prozesse eines Rechners einzuholen.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Entwurf/entwurf_prozesse_gruppen.png}
    \caption{Entwurf der Anzeige der einzelnen Prozesse für die Gruppenanzeige}
\end{figure}


\section{Architektur}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{UC/LogSense_Architektur.png}
    \caption{Programmarchitektur}
\end{figure}
\subsection{Agent}
Der Agent ist eine lokale Java-Applikation die auf dem installierten Rechner bei welchem die Hardware-Metriken gemessen werden im Hintergrund läuft .Dieser ist dafür zuständig, die Hardware-Allozierungsdaten und die Verwendungsdauer von Prozessen mithilfe der OSHI-Bibliothek aus dem Rechner auszulesen, diese in Applikationen zu gruppieren und anschließend im geeigneten CSV-Format zur REST-API zu senden.
\subsection{REST-API}
Die Rest-API wird auf einem Server ausgeführt und ist für die Kommunikation zwischen Webapplikation, Agent und den Algorithmen im Backend verantwortlich. Die verwendete Programmiersprache ist Python aufgrund den Verknüpfungen mit den Algorithmen. Bei dem verwendeten Web-Framework für die Restful-API handelt es sich um FastAPI. Der Zugriff auf die Datenbank erfolgt über den Datenbankadapter psycopg2.
\subsection{Datenbank}
Als Datenbank wird eine Instanz einer TimescaleDB verwendet. Die Datenbank ist verantwortlich für das Speichern der gemessenen Daten aus dem Agent, den angelegten Rechnern und den spezifizierten Custom-Alerts. 
\subsection{Algorithmen}
Die Algorithmen zur Datenanalyse- und verarbeitung sind in Python geschrieben und werden zur Erkennung von Anomalien, Change Points, Forecast und der Überprüfung von Custom Alerts verwendet. Um diese Zwecke zu erfüllen, erhält die Applikation Referenzen auf die Python-Bibliotheken Pandas, Numpy, Ruptures und Scikit-learn.
\subsection{Webapplikation}
Die Webapplikaton repräsentiert in LogSense die Benutzeroberfläche und bildet die Basis für die Interaktion des Benutzers mit der Gesamtapplikation. Bei der Webapplikatiion bekommt der Benutzer eine Übersicht über die Laufzeit der Applikationen und einen Verlauf inklusive Analysen von ihren CPU und RAM-Auslastungen. Weiters kann der Benutzer den Verlauf des freien Speichers ansehen mit der Zusatzmöglichkeit, Prognosen über den zukünftigen Verlauf zu bekommen. Auf jeder aufgerufenen Seite erhält der Benutzer relevante Statistiken und eine Liste von aufgetretenen Alerts. Auf der Custom-Alert Seite kann dieser neue Alerts hinzufügen, bestehende Alerts löschen oder bearbeiten. 


\section{Informationsfluss}
Beim Starten des Agents werden die allgemeinen Daten über den Rechner mittels eines HTTP Clients an den REST Endpoint des Servers gesendet, um den Beginn einer Datenerfassung zu kennzeichnen. Wenn dieser Schritt erfolgreich durchgeführt worden ist, werden alle 60 Sekunden die Daten über den Ressourcenverbrauch seit der letzten Übertragung zusammengefasst, indem die durchschnittlichen Werte berechnet werden. Die Durchschnittswerte werden dann an den Server zur weiteren Verarbeitug gesendet.
Alle bereits vollständigen Daten werden vom Server ohne Änderung in der Datenbank persistiert. Die restlichen Daten werden vor der Speicherung noch transformiert, um alle logisch zusammenhängenden Daten gemeinsam persistieren zu können.
Der Server stellt verschiedene REST Endpoints zur Verfügung, welche die Daten aus der Datenbank abfragen, analysieren und auswerten.
Diese REST Endpoints werden vom Frontend aufgerufen, um Informationen über den Rechner und dessen Ressourcenverbrauch zu erhalten und in weiterer Folge darzustellen.

\chapter{Implementierung (Programmierung und QS/Test)}
\section{Visualiserung}
\section{Datenverarbeitung}
\subsection{Zusammenführung}
Da der Agent den Hardware-Verbrauch pro laufenden Prozess misst, die Applikation aber den Hardware-Verbrauch von Applikationen und den gesamten Rechner darstellt, müssen vor der Persistierung von Daten diese zunächst zusammengeführt werden. Zum Zweck der Bererechnung des gesamten PC-Verbrauchs wird die groupby-Methode von Pandas verwendet welche Daten pro Timestamp zusammenführt und die einzelnen Werte dabei summiert.
\subsection{Anomalienerkennung}
Ein Ziel der Anwendung ist es, bei den PC- und Anwedungsdaten Extremwerte und Abweichungen zu identifzieren und diese für den Benutzer zu markieren. Anomalienerkennung bei dieser Diplomarbeit hat den Sinn, ungewöhnliches Verhalten von Applikationen den Benutzer aufzuzeigen und anhand dessen ihn bei der Erkennung von Problemen beim Rechner behilflich zu sein. In Kombination mit Justifications bekommt dieser Hinweise darauf, weswegen diese Extremwerte zustandekommen. Auch allgemein spielt die Erkennung von Extremwerten bzw. Anomalien eine große Rolle im Datenverarbeitungsbereich, hauptsächlich aufgrund der Bereinigung von Datensätzen.
\subsubsection{Implementierungsmöglichkeiten}
Für das Erkennen von Extremwerten bieten sich eine Vielzahl von Möglichkeiten an, dazu zählen etwa das Aufteilen der Datensätze in Quantile, das Verwenden von Machine-Learning Algorithmen wie Isolation Forest und K-means oder das Anwenden von Stastisischen Methoden in Verbindung mit eigenen Algorithmen. 
\subsubsection{Händische Implementierung}
Im ersten Schritt werden Extremwerte anhand von bestimmten, prozentuellen Abweichungen vom gleitenden Durschnitt erkannt worden.Zunächst wird dafür der dafür der gleitende Durschnitt von Datensätzen berechnet worden und dann die prozentuelle Änderung von einem Datenpunkt zum gleitenden Durschnitt. Überschreitet dieser Wert eine definierte Grenze, beispielsweise 30 Prozent überschreiten, wird der Datenpunkt als Extremwert identifiziert.
Diese Vorgehensweise ist zwar ausreichend für das Erkennen von den meisten Extremwert-Punkten, jedoch nicht geeignet für Daten welche hohe Varianz aufweisen. \todo{Konkretes, praktisches beispiel}
\subsubsection{Anomalienerkennung mit Isolation Forest}
Zur Anomalienerkennung ist der Machine-Learning-Algorithmus Isolation Forest zur Verwendung gekommen. Dieser basiert auf das Konzept der Isolation, also der Tatsache, dass Anomalien weniger isoliert sind als herkömmliche Datenpunkte. 
\citet{isolation_forest} \\
Die Anomalienerkennung mit dem Machine-Learning-Algorithmus Isolation Forest hat gegenüber der Händischen Implementierung und statistischen Methoden wie Z-Scores den Vorteil, dass es keinerleine Annahmen über die Verteilung unserer Daten benötigt und problemlos mit Daten hoher Varianz arbeiten kann.
Bei der Implementierung sind Probleme bezüglich Underfitting aufgretreten falls nur eine geringe Menge an Datensätzen zum Trainieren des Algorithmus verfügbar gewesen ist. Dies ist der Fall, wenn der Benutzer die Datenanalyse in einem Zeitrahmen anfordert, wo nur eine geringe Menge an Datensätzen verfügbar ist. Als Lösung sind zum Tranieren des Algorithmus nicht nur die Daten im gewählten Zeitrahmen genommen worden, sondern auch vorherliegende Datensätze. 


\begin{figure}[H]
    \centering
     \includegraphics[width=0.9\textwidth]{DataScience/anomaly_detection_2.png}
    \caption{Aufruf von detect anomalies}
\end{figure}
Bei Aufruf der Funktion müssen ein DataFrame zur Vorhersage von Anomalien und ein DataFrame zum Trainieren des Models übergeben werden. In den meisten Fällen weisen beide DataFrames die gleichen Daten auf, außer es kommt zum angesprochenen Problem, dass zu wenige Trainingsdaten vorhanden sind. In diesem Fall wird eine Funktion aufgerufen, die von der Datenbank die letzten 100 Datenpunkte anfordert. Die gewählten Werte sind zwar arbitär, allerdings weisen sie gute Ergebnisse bei Tests auf. Als letzter Parameter wird der Name der Spalte übergeben, in diesem Beispiel wird die gewählte Spalte als "value" aus der Datenbank selektiert. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{DataScience/anomaly_detection.png}
    \caption{Algorithmus für die Anomalienerkennung}
\end{figure}
Nach Aufruf der Funktion müssen die betroffenen Daten aus den DataFrames extrahiert werden, als Übergabewert verlangt der IsolationForest-Algorithmus nämlch ein numpy-Array. Mit .values von Pandas wird die betroffene Column des DataFrames als Series ausgewählt und anschließend mit reshape(-1,1) in eine Single-Column umgewandelt. Danach wird das Model trainiert mit einer festgesetzten Kontaminierungsrate von 0.03. Die Kontaminierungsrate gibt den Prozentsatz der erwarteten Anomalien an, in diesem Fall ist der Prozentsatz von 3 gewählt worden, da dieser Wert bei Versuche die besten Ergebnisse erzielt hat. Da das Ergebnis reproduzierbar sein sollte, wird ein fixer Random State von 42 gesetzt. Die Wahl dessen hat keinerlei Einfluss auf die Qualität der Anomalieerkennung und dient nur für konstante Ergebnisse bei gleichen Daten. Ohne einem konkreten Random State besteht ansonsten die Möglichkeit, dass bei identen Datenbestände verschiedene Anomalien pro Request erkannt werden. Der Algorithmus markiert alle Anomalien mit -1 und alle herkömmlichen Datenpunkte mit 1, daher werden mithilfe von .where der numpy-Bibliothek die Anomalie-Datepunkte selektiert bei dem die predicted labels -1 betragen. Die Anomalienerkennung ist damit abgeschlossen, allerdings besteht die erzeugte Liste nur aus den Indizes der Datenpunkte bei denen eine Anomalie aufgetreten ist. Damit unsere restlichen Algorithmen leichter mit den Daten arbeiten können, werden die Messzeiten der Indizes aus der Prediction-DataFrame ausgewählt und zurückgegeben. 
\subsection{Change Point Detection}
Zusätzlich zur Erkennung von Anomalien benötigt LogSense auch das Erkennen von Change Points zur Identifizierung von Events und zum Feststellen von Trends zwischen den Change Points. Change Points sind Zeitpunkte in einer Zeitreihe, an denen sich das Verhalten der Daten oder statistische Eigenschaften signifikant ändern, beispielsweise durch einen enormen Anstieg oder Abfall der Daten. Für diese Aufgabe wird der Pelt (Pruned Exact Linear Time)-Algorithmus aus der ruptures-Bibliothek angewendet. Change-Point-Detection-Algorithmen teilen die Zeitreihe in mehrere Segmente auf und messen die Effektivität der möglichen Aufteilungen. Pelt bestraft das Modell für jeden zusätzlich gefundenen Change-Point, indem zu den Kosten eines Segments der Bestrafungswert addiert wird. Dann wählt der Algorithmus die Segmentierung aus, bei der diese Kosten am geringsten sind. Der Grundgedanke dabei ist, dass ein Datenpunkt als Change-Point erkannt wird, wenn er die Segmentierungskosten mehr verringert als der Bestrafungswert sie erhöht. Dies ist genau dann der Fall, wenn sich das Verhalten der Daten enorm ändert. Dieser Bestrafungswert muss je nach Beschaffenheit der Daten angepasst werden, wenn dieser zu niedrig ist, werden viele False Positives erkannt und umgekehrt.
\citet{change_point_explanation}
\subsection{Forecast}
Ein weiteres Feature von LogSense ist die Möglichkeit, den zukünftigen Verlauf von Zeitserien vorherzusagen. Zwar kann man dies in der Theorie auf alle unsere Daten anwenden, sinnvoll ist es allerdings nur bei der Vorhersage von freien Speicherplatz da andere Metriken wie etwa RAM- oder CPU-Auslastung keine wirkliche Trends aufweisen. 
\subsubsection{ARIMA}
\subsubsection{Lineare Regression}

Zur Umsetzung von Vorhersagen wurde die statistische Methode der Linearen Regression verwendet. Das Ziel der Linearen Regression ist es, einen abhängigen Wert \( Y \) durch einen unabhängigen Wert \( X \) vorherzusagen. Voraussetzung ist, dass zwischen den beiden Werten ein linearer Zusammenhang besteht und es sich um stetige Werte handelt. In unserem Fall ist der abhängige Wert der freie Speicher und der unabhängige Wert der gemessene Zeitpunkt.\newline

 
\textbf{Formel:} \( y \) (geschätzte abhängige Variable) = \( b \) (Steigung) * \( x \) (unabhängige Variable) + \( a \) (y-Achsenabschnitt)\newline


Im Normalfall wird unser freier Speicher stets sinken durch das Installieren von zusätzlichen Applikationen und mehr gespeicherten Daten, daher ergibt sich eine Korrelation zwischen Speicher und Zeit sowie ein stetiger Verlauf des freien Speichers. Allerdings besteht die Möglichkeit, dass der Benutzer am Rechner Daten löscht oder Programme deinstalliert und sich daher der Trend verändert.
Aus diesem Grund werden vor dem Bilden der linearen Regression zunächst alle Change-Points in der Zeitreihe bestimmt und die Daten des letzten Segments zum Trainieren des Linear-Regression-Models verwendet. Dadurch wird auch immer nur der aktuellste Trend beachtet. Jedoch dürfen nicht zu viele Change-Points erkannt werden, da ansonsten einfache Schwankungen bereits einen neuen Trend bedeuten. Es ist also ein hoher Penalty-Value verwendet worden.\newline


Nach Erstellung des trainierten Modells werden anhand dessen die Werte bei zukünftigen Zeitpunkten vorhergesagt. Da das Feature hauptsächlich der Vorhersage dessen dient, wann unser Speicher aufgebracht wird, werden die vorhergesagten Werte tagesweise gruppiert und Vorhersagen nur bis zu einer bestimmten Grenze von Tagen getroffen (angegeben durch den Benutzer per API-Request).
Ohne einer bestimmten Grenze besteht sonst das Risiko, dass Tausende von Tagen vergehen müssen, damit der Speicher aufgebraucht ist und daher der Request ungewöhnlich lange benötigt. Auch wird die Vorhersage von Werten abgebrochen, sobald der Wert negativ ist, da solche Vorhersagen dem Benutzer keinen Nutzen mehr bringen.
\subsection{Userdefinierte Alerts}

\subsection{Justifications}

\section{Datenerfassung}
\subsection{Rechner-Daten}
\subsection{Ressourcenverbrauch vom Rechner und einzelnen Applikationen}
\subsection{Vordergrundzeit einer Applikation}
\subsection{Zusammenführen der gemessen Daten}
\subsection{CSV Konverter}
\subsection{REST Client}
\subsubsection{Rechner-Daten}
\subsubsection{Ressourcenverbrauch}
\subsection{Programmablauf}
Der Agent soll alle 10 Sekunden den Ressourcenverbrauch erfassen. Das heißt, dass die Methode zur Überwachung des Ressourcenvebrauch nach Ablauf einer gewissen Zeitspanne aufgerufen werden muss. Um dies zu erreichen, können nachfolgende 2 Varianten verwendet werden.
\subsubsection{ScheduledExecutorService}
Der ScheduledExecuterService ist Teil des java.util.concurrent Package, das Möglichkeiten bietet Anwendungen parallel auszuführen.\\\\
\textbf{java.util.concurrent Package}\\\\
Dieses Package ist mit der Java Version 5 eingeführt worden und ermöglicht eine gleichzeitige Ausführung von mehreren Prozessen in einer Java-Anwendung. Das ist vor allem bei Webandwendungen von Vorteil, da neuere Interaktionen mit der Benutzeroberfläche weiterhin verarbeitet werden können, obwohl ältere Anfragen noch nicht fertig abgearbeitet worden sind. Zusätzlich können komplexe und ressourcenintensive Aufgaben wie Datei- und Datenbankzugriffe oder Netzwerkanfragen ausgeführt werden, während gleichzeitig andere Aufgaben erledigt werden können. Dadurch wird der Programmablauf nicht komplett blockiert, wenn ein Datenbankzugriff durchgeführt wird, der mehrere Sekunden dauert.\\\\
Wie der Name dieses Packages schon sagt, werden Programmteile gleichzeitig und nicht parallel ausgeführt. Eine parallele Ausführung würde bedeuten, dass verschiedene Prozesse zum selben Zeitpunkt von verschiedenen Kernen der CPU bearbeitet werden. Das ist nur möglich, wenn diese Prozesse unabhängig voneinander sind und nicht miteinander kommunizieren. Werden Aufgaben hingegen gleichzeitig ausgeführt, arbeitet die CPU an einem Prozess und wendet sich nach einer gewissen Zeit einem anderen Prozess zu. Das geschieht so schnell, dass es von außen betrachtet so aussieht, als würden die Prozesse parallel laufen.\\\\
Genau dieses Prinzip der Gleichzeitigkeit wird als Basis für folende 5 Hauptkomponenten des java.util.concurrent Package verwendet:
\begin{itemize}
    \item \textbf{Executors}
        \begin{description}
            \item \noindent 
        \end{description}
    \item \textbf{Queues}
        \begin{description}
            \item \noindent 
        \end{description}
    \item \textbf{Timing}
        \begin{description}
            \item \noindent 
        \end{description}
    \item \textbf{Synchronizers}
        \begin{description}
            \item \noindent 
        \end{description}
    \item \textbf{Concurrent Collections}
        \begin{description}
            \item \noindent 
        \end{description}
\end{itemize}
\subsubsection{Timer}
\subsubsection{Thread Sleep}

\section{Datenhaltung}
\subsection{Datenbank}
\subsubsection{Einleitung}
Die verwendete Datenbank ist TimescaleDB, welche auf Postgres aufbaut, der Vorteil dessen ist es, dass sowohl relationale Datenstrukturen, als auch die für Timeseries Data optimierten, sogenannten "Hypertable" zur verfügung stehen. Ein Vorteil dessen ist, dass TimescaleDB schnelle Lese -und Schreibgeschwindigkeiten für große Mengen an Timeseries-Daten anbietet, etwa wie die vom Agent gemessen Datensätze.

\title{Warum wird eine Datenbank benötigt?}
Die vom Agent gemessen Daten müssen für spätere analyse und visualisierung persistiert werden. Weiters wird die wird die Datenbank zum abspeichern von Benutzerinformationen verwendet.

\subsubsection{Datenmodell}
\subsubsection{Datenkatalog}
Der Datenkatalog beschreibt die einzelnen Tabellen, deren Funktion und die jeweiligen Attribute.
\subsection{Einfügen in Datenbank}
\section{Schnittstellen}
\section{Server}


\chapter{Ergebnis}
\section{Statistiken und Charts}
\section{Anomalien}
\section{Alerts}
\section{Ausblick}

\chapter{Resümee}


\chapter*{Quellverzeichnis}
Pandas \citep{pandas_docs}.

\chapter*{Abbildungsverzeichnis}

\chapter*{Glossar}

\chapter*{Anhang}
\section*{Architekturskizze}
\section*{Datenmodell}
\section*{Diplomarbeitsplakat}
\section*{Abnahmeformular}

\end{document}