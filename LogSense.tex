\documentclass{report}
\usepackage{titlesec}

\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries}{}{0pt}{\Huge}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{todonotes}
\usepackage{babel}
\usepackage[numbers,comma,square]{natbib}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{grffile}

\pagestyle{fancy}
\fancyhf{}
\rhead{LogSense}
\lhead{HTL Perg}
\renewcommand{\headrulewidth}{0pt}



\fancypagestyle{plain}{
  \fancyhf{}
  \rhead{LogSense}
  \lhead{HTL Perg}
  \fancyfoot[R]{\thepage}
  \fancyfoot[L]{Borbely, Ettlinger, Jilek, Stadlbauer}
  \renewcommand{\headrulewidth}{0pt}
}


\fancyfoot{}
\fancyfoot[R]{\thepage} % Page number format on the right side
\fancyfoot[L]{Borbely, Ettlinger, Jilek, Stadlbauer} % Names on the left side
\begin{document}

\begin{figure}[htp]
    \centering
    \includegraphics[width=4cm, height=4cm]{LatexStyleFiles/htl_perg.png}
    \caption{Graph mit erkannten Anoalien}
\end{figure}

\tableofcontents

\chapter*{Eidesstattliche Erklärung}
This chapter introduces the background and motivation behind the software engineering project.

\chapter*{Gender-Erklärung}
Define the scope of the project, including any limitations and constraints.

\chapter*{Danksagung}
Summarize relevant literature and related work in the field of software engineering.

\chapter*{Impressum}
Clearly state the problem the project aims to address.

\chapter*{Kurzfassung}
Define the specific objectives and goals of the software engineering project.
\textbf{Problemstellung} 

\chapter*{Abstract}
Define the specific objectives and goals of the software engineering project.

\chapter*{Inhaltsverzeichnis}

\chapter{Einleitung}
\section{Ausgangssituation}
\section{Problemstellung}
\section{Zielsetzung}
\section{Projektinhalt}
\subsection{Anforderungen}
\subsection{Projektteam}
\subsubsection{Aufgabenverteilung}
\subsection{Auftraggeber}

\chapter{Theoretische und fachpraktische Grundlagen und Methoden}
\section{Visualiserung}
\subsection{Angular}
\subsection{Charst JS}
\text
Chart.js ist eine JavaScript Bibliothek mit deren Hilfe man unterschiedliche Diagrammtypen in Webanwendungen darstellen kann. 
Es können 8 unterschiedliche Diagrammarten dargestellt werden, darunter: 
\begin{itemize}
    \item Liniendiagramme
    \item Balkendiagramme
    \item Bereichsdiagramme
    \item Kreisdiagramme
    \item Tortendiagramme
    \item Radardiagramme
    \item Polar Diagramme
    \item Streudiagramme
\end{itemize}
Die Bibliothek bietet unterschiedlieche, interaktive Funktionen wie Tooltipps und Animationen. 
Darüber hinaus gibt es viele Möglichkeiten mit verschiedenen Optionen und Konfigurationen das Diagramm nach eigenen Wünschen  anzupassen. 
Chart.js unterstützt responsives Design und kann sich an unterschiedliche Bildschirmgrößen anpassen.\\In dieser Diplomarbeit ist Chart.js im Angular Projekt eingebunden um unterschiedliche Messwerte grafisch darzustellen.
\subsection{BootStrap}
\subsection{Angular Material}

\section{Datenverarbeitung}
\subsection{Python}
\subsection{Pandas}
Pandas ist eine Python-Bibliothek zur Datenverarbeitung und -analyse. Sie stellt verschiedene Datenstrukturen und Funktionen für das Manipulieren von tabellarischen und strukturierten Daten zur Verfügung. Zu den wichtigsten Strukturen gehören DataFrames und Series.
\begin{itemize}
    \item \textbf{Series} \\
    \begin{minipage}[t]{\linewidth}
        Series sind eindimensionale, array-ähnliche Objekte, die wie eine einzelne Spalte in einer Tabelle angeordnet sind.
    \end{minipage}

    \item \textbf{DataFrames} \\
    \begin{minipage}[t]{\linewidth}
        Bei DataFrames handelt es sich um zweidimensionale Tabellen mit Zeilen und Spalten. Einzelne Spalten von einem DataFrame können dabei verschiedene Datentypen aufweisen. Zu den wichtigsten Funktionen gehören Aggregationen, Gruppierungen und Statistische Operationen. 
    \end{minipage}
\end{itemize}
  In der vorliegenden Diplomarbeit wird Pandas bei der Datenverarbeitung eingesetzt, insbesondere in Kombination mit Scikit-Learn, Numpy und Ruptures.
  Features bei denen Pandas eingesetzt wird:
  \begin{itemize}
      \item Anomalien
      \item Change-Points
      \item Bereinigung von Datensätzen
      \item Aggregation von Datensätzen
  \end{itemize}

\bibliographystyle{plain}
\bibliography{data_science}

\subsection{Numpy}
\subsection{Scikit-Learn}
\subsection{Ruptures}
\subsection{Algorithmen}

\section{Datenerfassung}
\subsection{Agent}
Für die Erfassung der Ressourcendaten wird ein sogennanter Agent verwendet. Dieser kann selbstständig, das heißt ohne das Zutun eines Benutzers oder eines anderen Programmes, Tätigkeiten und Prozesse ausführen. Das heißt, der Agent wartet so lange bis eine gewisse Zeit abgelaufen, ein Programm gestartet worden oder ein Ereignis jeglicher Art festgestellt worden ist und erledigt darauf bestimmte Aufgaben.\\
Dabei gibt es im wesentlichen 3 Arten von Agenten:
\begin{figure}
\centering
\includegraphics[width=0.25\linewidth]{frog.jpg}
\caption{\label{fig:frog}This frog was uploaded via the file-tree menu.}
\end{figure}

\begin{itemize}
    \item Reaktive Agenten
        \begin{description}
            \item Reaktive Agenten beobachten die Umgebung, auf der sie laufen und treffen Entscheidungen basierend auf den erfassten Daten.
        \end{description}
    \item Adaptive Agenten
        \begin{description}
            \item Diese Art von Agenten handelt, im Vergleich zu reaktiven Agenten, zusätzlich basierend auf bereits zuvor erfassten Daten und erkannten Zusammenhängen zwischen den Informationspunkten.
        \end{description}
    \item Kognitive Agenten
        \begin{description}
            \item Kognitive Agenten können weiters aus den erfassten Daten Muster lernen und selbstständig abwägen, welche Aufgaben wann und wie durchgeführt werden sollen, um bestmöglich auf die aktualle Situation der Umgebung reagieren zu können.
        \end{description}
\end{itemize}
Für diesen Anwendungsfall wird ein reaktiver Agent verwendet, da nach Ablauf von 60 Sekunden die Daten gemessen, je nach Art der Daten zusammengefasst und anschließend in einem geeigneten Format zur Auswertung an den Server gesendet werden sollen. Es wird also überprüft, um welche Daten es sich handelt und daraufhin entschieden wie die Daten für das Senden vorbereitet werden sollen.

\subsection{Java}
Die Programmiersprache Java ist objektorientiert, besitzt eine einfache Syntax mit überschaubarem Sprachumfang und ist durch kontrollierte Speicherzugriffe sicher. Wenn es eine passende JVM (= Java Virtual Machine) für das Betriebssystem gibt, kann das Programm auf diesem Betriebssytem laufen. Diese Plattformunabhängig wird dadurch ermöglicht, dass der Java-Quellcode in Bytecode übersetzt wird, der dann von der JVM interpretiert werden kann.\\
Der Agent, der lokal auf den Windows Rechnern läuft und die Daten über den Ressourcenverbrauch des Rechners erfasst, ist in der Programmiersprache Java entwickelt. Der Grund dafür ist, dass es für Java viele verschiedene Bibliotheken gibt, die auf die Hardwaretreiber zugreifen, um die benötigten Daten über den Rechner, die darauf laufenden Programme und deren Ressourcenverbrauch auszulesen.

\subsection{Oshi}

\section{Datenhaltung}
\subsection{Postgres}
\subsection{Timescale DB}

\section{Schnittstellen}
\subsection{Fast API}
\subsection{REST}

\section{Server}
\subsection{Ubuntu}
\subsection{nginx}

\section{Entwicklungssysteme}
\subsection{PyCharm}
\subsection{IntelliJ}
\subsection{Webstorm}
\subsection{Datagrip}
\section{Sonstige Software}
\subsection{GitLab}
\subsection{Adobe XD}
\subsection{LucidChart}

\chapter{Planung und Realisierung}
\section{Funktionalität (UC, UC-Beschreibung)}
\section{Architektur}
\section{Informationsfluss}
\section{Datenmodell}

\chapter{Implementierung (Programmierung und QS/Test)}
\section{Visualiserung}
\section{Datenverarbeitung}
\subsection{Zusammenführung}
Da der Agent den Hardware-Verbrauch pro laufenden Prozess misst, die Applikation aber den Hardware-Verbrauch von Applikationen und den gesamten Rechner darstellt, müssen vor der Persistierung von Daten diese zunächst zusammengeführt werden. Zum Zweck der Bererechnung des gesamten PC-Verbrauchs wird die groupby-Methode von Pandas verwendet welche Daten pro Timestamp zusammenführt und die einzelnen Werte dabei summiert.
\subsection{Anomalienerkennung}
Ein Ziel der Anwendung ist es, bei den PC- und Anwedungsdaten Extremwerte und Abweichungen zu identifzieren und diese für den Benutzer zu markieren. Anomalienerkennung bei dieser Diplomarbeit hat den Sinn, ungewöhnliches Verhalten von Applikationen den Benutzer aufzuzeigen und anhand dessen ihn bei der Erkennung von Problemen beim Rechner behilflich zu sein. In Kombination mit Justifications bekommt dieser Hinweise darauf, weswegen diese Extremwerte zustandekommen. Auch allgemein spielt die Erkennung von Extremwerten bzw. Anomalien eine große Rolle im Datenverarbeitungsbereich, hauptsächlich aufgrund der Bereinigung von Datensätzen.
\subsubsection{Implementierungsmöglichkeiten}
Für das Erkennen von Extremwerten bieten sich eine Vielzahl von Möglichkeiten an, dazu zählen etwa das Aufteilen der Datensätze in Quantile, das Verwenden von Machine-Learning Algorithmen wie Isolation Forest und K-means oder das Anwenden von Stastisischen Methoden in Verbindung mit eigenen Algorithmen. 
\subsubsection{Händische Implementierung}
Im ersten Schritt werden Extremwerte anhand von bestimmten, prozentuellen Abweichungen vom gleitenden Durschnitt erkannt worden.Zunächst wird dafür der dafür der gleitende Durschnitt von Datensätzen berechnet worden und dann die prozentuelle Änderung von einem Datenpunkt zum gleitenden Durschnitt. Überschreitet dieser Wert eine definierte Grenze, beispielsweise 30 Prozent überschreiten, wird der Datenpunkt als Extremwert identifiziert.
Diese Vorgehensweise ist zwar ausreichend für das Erkennen von den meisten Extremwert-Punkten, jedoch nicht geeignet für Daten welche hohe Varianz aufweisen. \todo{Konkretes, praktisches beispiel}
\subsubsection{Anomalienerkennung mit Isolation Forest}
Zur Anomalienerkennung ist der Machine-Learning-Algorithmus Isolation Forest zur Verwendung gekommen. Dieser basiert auf das Konzept der Isolation, also der Tatsache, dass Anomalien weniger isoliert sind als herkömmliche Datenpunkte. 
Die Anomalienerkennung mit dem Machine-Learning-Algorithmus Isolation Forest hat gegenüber der Händischen Implementierung und statistischen Methoden wie Z-Scores den Vorteil, dass es keinerleine Annahmen über die Verteilung unserer Daten benötigt und problemlos mit Daten hoher Varianz arbeiten kann.
Bei der Implementierung sind Probleme bezüglich Underfitting aufgretreten falls nur eine geringe Menge an Datensätzen zum Trainieren des Algorithmus verfügbar gewesen ist. Dies ist der Fall, wenn der Benutzer die Datenanalyse in einem Zeitrahmen anfordert wo nur eine geringe Menge an Datensätzen verfügbar ist. Als Lösung sind zum Tranieren des Algorithmus nicht nur die Daten im gewählten Zeitrahmen genommen worden, sondern auch vorherliegende Datensätze. 
\subsection{Change Point Detection}
Zusätzlich zur Anomalieerkennung benötigt LogSense auch das Erkennen von Change Points zur Identifizierung von Events und zum Feststellen von Trends zwischen den Change Points. Change Points sind Zeitpunkte in einer Zeitreihe, an denen sich das Verhalten der Daten oder statistische Eigenschaften signifikant ändern, beispielsweise durch einen enormen Anstieg oder Abfall der Daten. Für diese Aufgabe wird der Pelt (Pruned Exact Linear Time)-Algorithmus aus der ruptures-Bibliothek angewendet. Change-Point-Detection-Algorithmen teilen die Zeitreihe in mehrere Segmente auf und messen die Effektivität der möglichen Aufteilungen. Pelt bestraft das Modell für jeden zusätzlich gefundenen Change-Point, indem zu den Kosten eines Segments der Bestrafungswert addiert wird. Dann wählt der Algorithmus die Segmentierung aus, bei der diese Kosten am geringsten sind. Der Grundgedanke dabei ist, dass ein Datenpunkt als Change-Point erkannt wird, wenn er die Segmentierungskosten mehr verringert als der Bestrafungswert sie erhöht. Dies ist genau dann der Fall, wenn sich das Verhalten der Daten enorm ändert. Dieser Bestrafungswert muss je nach Beschaffenheit der Daten angepasst werden, wenn dieser zu niedrig ist, werden viele False Positives erkannt und umgekehrt.
\subsection{Forecast}
Ein weiteres Feature von LogSense ist die Möglichkeit, den zukünftigen Verlauf von Zeitserien vorherzusagen. Zwar kann man dies in der Theorie auf alle unsere Daten anwenden, sinnvoll ist es allerdings nur bei der Vorhersage von freien Speicherplatz da andere Metriken wie etwa RAM- oder CPU-Auslastung keine wirkliche Trends aufweisen. 
\subsubsection{ARIMA}
\subsubsection{Lineare Regression}

Zur Umsetzung von Vorhersagen wurde die statistische Methode der Linearen Regression verwendet. Das Ziel der Linearen Regression ist es, einen abhängigen Wert \( Y \) durch einen unabhängigen Wert \( X \) vorherzusagen. Voraussetzung ist, dass zwischen den beiden Werten ein linearer Zusammenhang besteht und es sich um stetige Werte handelt. In unserem Fall ist der abhängige Wert der freie Speicher und der unabhängige Wert der gemessene Zeitpunkt.\newline

 
\textbf{Formel:} \( y \) (geschätzte abhängige Variable) = \( b \) (Steigung) * \( x \) (unabhängige Variable) + \( a \) (y-Achsenabschnitt)\newline


Im Normalfall wird unser freier Speicher stets sinken durch das Installieren von zusätzlichen Applikationen und mehr gespeicherten Daten, daher ergibt sich eine Korrelation zwischen Speicher und Zeit sowie ein stetiger Verlauf des freien Speichers. Allerdings besteht die Möglichkeit, dass der Benutzer am Rechner Daten löscht oder Programme deinstalliert und sich daher der Trend verändert.
Aus diesem Grund werden vor dem Bilden der linearen Regression zunächst alle Change-Points in der Zeitreihe bestimmt und die Daten des letzten Segments zum Trainieren des Linear-Regression-Models verwendet. Dadurch wird auch immer nur der aktuellste Trend beachtet. Jedoch dürfen nicht zu viele Change-Points erkannt werden, da ansonsten einfache Schwankungen bereits einen neuen Trend bedeuten. Es ist also ein hoher Penalty-Value verwendet worden.\newline


Nach Erstellung des trainierten Modells werden anhand dessen die Werte bei zukünftigen Zeitpunkten vorhergesagt. Da das Feature hauptsächlich der Vorhersage dessen dient, wann unser Speicher aufgebracht wird, werden die vorhergesagten Werte tagesweise gruppiert und Vorhersagen nur bis zu einer bestimmten Grenze von Tagen getroffen (angegeben durch den Benutzer per API-Request).
Ohne einer bestimmten Grenze besteht sonst das Risiko, dass Tausende von Tagen vergehen müssen, damit der Speicher aufgebraucht ist und daher der Request ungewöhnlich lange benötigt. Auch wird die Vorhersage von Werten abgebrochen, sobald der Wert negativ ist, da solche Vorhersagen dem Benutzer keinen Nutzen mehr bringen.
\subsection{Userdefinierte Alerts}
\subsection{Justifications}
\section{Datenerfassung}
\section{Datenhaltung}
\subsection{Datenbank}
\subsubsection{Einleitung}
Die verwendete Datenbank ist TimescaleDB, welche auf Postgres aufbaut, der Vorteil dessen ist es, dass sowohl relationale Datenstrukturen, als auch die für Timeseries Data optimierten, sogenannten "Hypertable" zur verfügung stehen. Ein Vorteil dessen ist, dass TimescaleDB schnelle Lese -und Schreibgeschwindigkeiten für große Mengen an Timeseries-Daten anbietet, etwa wie die vom Agent gemessen Datensätze.

\title{Warum wird eine Datenbank benötigt?}
Die vom Agent gemessen Daten müssen für spätere analyse und visualisierung persistiert werden. Weiters wird die wird die Datenbank zum abspeichern von Benutzerinformationen verwendet.

\subsubsection{Datenmodell}
\subsubsection{Datenkatalog}
Der Datenkatalog beschreibt die einzelnen Tabellen, deren Funktion und die jeweiligen Attribute.
\subsection{Einfügen in Datenbank}
\section{Schnittstellen}
\section{Server}


\chapter{Ergebnis}
\section{Statistiken und Charts}
\section{Anomalien}
\section{Alerts}
\section{Ausblick}

\chapter{Resümee}


\chapter*{Quellverzeichnis}
Pandas \citep{pandas_docs}.

\chapter*{Abbildungsverzeichnis}

\chapter*{Glossar}

\chapter*{Anhang}
\section{Architekturskizze}
\section{Datenmodell}
\section{Diplomarbeitsplakat}
\section{Abnahmeformular}

\end{document}



